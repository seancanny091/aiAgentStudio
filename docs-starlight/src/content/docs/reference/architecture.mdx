---
title: Architecture
description: Runtime architecture and orchestration model.
---

import { Aside, Card, CardGrid, Steps, Tabs, TabItem } from '@astrojs/starlight/components';

# Architecture Overview

AI Agent Studio is built on a modular, plugin-based architecture using Strategy, Factory, and Chain of Responsibility patterns. The framework separates concerns across orchestration, execution, memory, security, and observability layers.

## Core Components

<CardGrid>
  <Card title="Orchestrators" icon="seti:settings">
    **Four execution patterns**: Conversational (multi-turn chat), Function (single-task with sync/async routing), Workflow (multi-agent state machines), Email (thread processing with auto-reply).
    
    All extend `BaseAgentOrchestrator` and implement `IAgentOrchestrator` interface.
  </Card>
  <Card title="LLM Providers" icon="seti:cloud">
    **Multi-provider support**: `OpenAIProviderAdapter` ships in core and works with OpenAI and any OpenAI-compatible API. `VertexAIProviderAdapter` for Google Vertex AI / Gemini is in addons. Implement `ILLMProviderAdapter` (extend `BaseProviderAdapter`) to add any other provider.
    
    `BaseProviderAdapter` provides common HTTP handling, retry logic, error normalization, and default `null` safety pre-check.
  </Card>
  <Card title="Actions" icon="seti:tools">
    **Standard actions**: GetRecordDetails, CreateRecord, UpdateRecord, FlowHandler, PostChatter.
    
    Extend `BaseAgentAction` implementing `IAgentAction` interface.
  </Card>
  <Card title="Context Providers" icon="seti:database">
    **Dynamic data enrichment**: Supply related records, user context, and computed data to agents.
    
    Implement `IAgentContextProvider` interface with bulk-safe, security-enforced queries.
  </Card>
</CardGrid>

## Execution Lifecycle

<Steps>

1. **Request Entry**
   
   Request enters via `AgentExecutionService.startExecution()` (invocable from Flow/Apex) or REST API endpoint `/services/apexrest/ai/agent/process/`.

2. **Orchestrator Routing**
   
   `AgentExecutionService` routes by agent type to the appropriate orchestrator (Conversational, Function, Workflow, or Email).

3. **Security Checks**
   
   **Prompt Safety**: `LLMInteractionService` calls `ILLMProviderAdapter.checkMessageSafety()` before the LLM call. Safety is provider-native—each adapter performs its own pre-check (e.g., OpenAI calls `/v1/moderations`). `BaseProviderAdapter` returns `null` by default (no pre-check). Skipped entirely when `PromptSafetyPreset__c` is `Off`.
   
   **PII Masking**: `PIIMaskingService` applies hybrid masking (schema-based + pattern-based) with deterministic token replacement.

4. **Context Assembly**
   
   **Memory**: `ContextManagerService` retrieves conversation history via `IMemoryManager` implementations (BufferWindow or SummaryBuffer).
   
   **Dynamic Context**: `ContextResolverService` invokes registered `IAgentContextProvider` implementations to gather related data.
   
   **System Prompt**: `SystemPromptBuilder` assembles identity, instructions, context, and orchestrator-specific additions.

5. **LLM Inference**
   
   **Request Building**: `LLMInteractionService.prepareAndCallLLM()` formats messages and tools via `LLMFormattingService`.
   
   **Tool Reasoning**: If enabled, adds `_rationale` parameter to all tools for explainability.
   
   **Provider Call**: Routes to the `ILLMProviderAdapter` implementation named in `LLMConfiguration__c.ProviderAdapterClass__c` (e.g. `OpenAIProviderAdapter`, `VertexAIProviderAdapter`, or a custom adapter).
   
   **Response Parsing**: Normalizes provider-specific responses into `ProviderResult`.

6. **Response Processing**
   
   `OrchestrationService.processLlmResult()` selects handler based on response type:
   
   - **Tool Calls**: `ToolCallResponseHandler` validates dependencies (if enabled), executes tools, manages async dispatch
   - **Content Only**: `ContentResponseHandler` processes text responses, handles email replies for EmailOrchestrator
   
   **Three-Tier Error Policy**: Capability FailFast → MaxRetries → Pass to LLM for recovery

7. **Action Execution**
   
   `CapabilityExecutionService` routes to action implementations:
   
   - **Standard**: Maps via `StandardActionHandler__mdt` to framework actions
   - **Apex**: Instantiates custom Apex class via `Type.forName()`
   - **Flow**: Invokes Flow via `Flow.Interview.createInterview()`
   
   **HITL**: If configured, routes through `HITLGatewayService` for approval workflows.
   
   **Security**: `TypeCoercionService.coerceArgumentTypesForSObject()` enforces FLS on field access.

8. **State Management**
   
   **Execution Records**: `AgentStateService` manages `AgentExecution__c` lifecycle with status tracking (Idle/Processing/Completed/Failed/Cancelled).
   
   **Step Logging**: Detailed execution trace captured per step — user inputs, LLM requests/responses, tool calls, tool results, and errors. Token counts and estimated cost recorded per step.
   
   **Storyboard Data**: High-level decision steps written for the Execution Storyboard UI, providing a user-friendly visual timeline.

9. **Async Continuation**
   
   After tool execution the framework determines whether to continue in the same transaction or defer to a new async job, depending on whether DML or callouts occurred. Long-running tools execute in a separate queueable.

10. **Completion**
    
    **Status Update**: Terminal state set (`Completed` or `Failed`). Any buffered writes are committed.
    
    **Events**: `AgentResponse__e` Platform Event published for conversational agents, triggering the UI to refresh.

</Steps>

## Key Services

<CardGrid>
  <Card title="AgentExecutionService" icon="rocket">
    Entry point for all execution types. Handles orchestrator routing via `AgentOrchestratorMapping__mdt` and service user context switching.
  </Card>
  <Card title="OrchestrationService" icon="seti:settings">
    LLM response processing and handler selection — `ToolCallResponseHandler` vs `ContentResponseHandler`. Three-tier error policy.
  </Card>
  <Card title="LLMInteractionService" icon="seti:cloud">
    LLM communication, system prompt assembly, PII masking, safety pre-check, and provider adapter routing.
  </Card>
  <Card title="CapabilityExecutionService" icon="seti:gear">
    Tool/action execution with handler factory for Standard, Apex, and Flow implementations. HITL gateway routing.
  </Card>
  <Card title="AgentStateService" icon="seti:db">
    Execution lifecycle management (`Idle → Processing → Completed/Failed`), status updates, and `AgentResponse__e` event publishing.
  </Card>
  <Card title="ContextManagerService" icon="seti:text">
    Conversation history retrieval via `IMemoryManager` (BufferWindow or SummaryBuffer). Context ledger management.
  </Card>
  <Card title="AIAgentConfigService" icon="seti:config">
    Configuration repository — agents, capabilities, context providers, and LLM configurations. Cached for performance.
  </Card>
  <Card title="PIIMaskingService" icon="seti:lock">
    Hybrid PII masking (Salesforce Data Classification + regex patterns). Deterministic token replacement with bidirectional unmask.
  </Card>
  <Card title="Async Dispatch" icon="seti:clock">
    Async job dispatch via Queueable or Platform Event, based on `AsyncDispatchType__c`. Handles both single-agent and multi-agent workflow dispatch.
  </Card>
  <Card title="StaleJobDetector" icon="warning">
    Scheduled recovery of stuck executions. Detects stalled jobs and re-queues them automatically.
  </Card>
</CardGrid>

## Configuration Objects

<Tabs>
  <TabItem label="AIAgentDefinition__c">

Core agent configuration:

**Identity & Behavior**: `AgentType__c` (Conversational/Function/Workflow/Email), `IdentityPrompt__c` and `InstructionsPrompt__c` (System prompt components), `EnableToolReasoning__c` (Require LLM to explain tool selection), `EnableParallelToolCalling__c` (Allow multiple tools per response).

**Memory & Context**: `LLMConfiguration__c` (LLM provider settings lookup), `MemoryStrategy__c` (Buffer Window/Summary Buffer), `HistoryTurnLimit__c` (Number of turns to remember).

**Security & Trust**: `PIIMaskingPreset__c` (Off/Standard/Strict), `PromptSafetyPreset__c` (Off/Standard/Strict).

**Performance**: `AsyncDispatchType__c` (High for Platform Events / Low for Queueable), `MaxProcessingCycles__c` (Max LLM turns per execution), `EnableDependencyValidation__c` (Enforce tool dependency graph).

**Workflow**: `ToolDependencyGraph__c` (JSON dependency graph for validation).

  </TabItem>
  <TabItem label="AgentCapability__c">

Tool/function definitions:

**Basic Config**: `CapabilityName__c` (Tool name exposed to LLM), `Description__c` (When and how to use this tool - critical for LLM guidance), `ImplementationType__c` (Standard/Apex/Flow), `StandardActionType__c` (Maps to StandardActionHandler__mdt), `ImplementationDetail__c` (Apex class name or Flow API name).

**Schema**: `Parameters__c` (JSON Schema for LLM defining expected arguments), `BackendConfiguration__c` (Admin config JSON passed to action implementation).

**Execution**: `RunAsynchronously__c` (Execute in separate queueable), `HITLMode__c` (Human-in-the-loop mode: Disabled/Confirmation/Approval/ConfirmationThenApproval), `ExposureLevel__c` (External visible to LLM / Internal framework only / Disabled). Error recovery is controlled at the agent level via `AIAgentDefinition__c.ErrorRecoveryMode__c` (Autonomous / Fail Fast).

  </TabItem>
  <TabItem label="AgentExecution__c">

Execution tracking:

**Status Fields**: `ExecutionStatus__c` (Idle/Processing/Completed/Failed/Cancelled for lifecycle-level), `ProcessingStatus__c` (Idle/Processing/Awaiting Action/Awaiting Followup/Failed for turn-level).

**Context**: `SourceRecordId__c` (Context record like Account or Case), `OriginalUserId__c` (User who initiated execution), `ServiceUserId__c` (Service user if RequiresServiceUserContext__c).

**Multi-Record Processing**: `BatchId__c` (Correlation ID for grouping related executions), `Priority__c` (Execution priority), `RetryCount__c`, `MaxRetries__c`, `NextRetryAt__c` (Exponential backoff tracking).

  </TabItem>
  <TabItem label="ExecutionStep__c">

Detailed execution log for observability:

**Step Metadata**: `StepType__c` (UserInput/AgentResponse/ToolCall/ToolResult/Error), `StepRole__c` (User/Assistant/Tool), `TurnIdentifier__c` (Groups steps by LLM turn).

**Content**: `Content__c` (Step payload/result data), `ToolRationale__c` (LLM reasoning for tool selection if EnableToolReasoning__c), `IsAsyncToolExecution__c` (Async execution flag).

**Token Economics**: `PromptTokens__c`, `CompletionTokens__c`, `TotalTokens__c`, `EstimatedCostUSD__c` (Cost tracking for budget monitoring).

  </TabItem>

</Tabs>

## Advanced Patterns

<CardGrid>
  <Card title="Efficient LLM Batching" icon="rocket">
    When tool execution doesn't require DML or external callouts, the framework can process multiple LLM turns within a single transaction, reducing latency and async job overhead.
  </Card>
  <Card title="Resilient Multi-Record Processing" icon="seti:flow">
    Multi-record batch executions use a resilient dispatch pattern with priority ordering, exponential backoff, progress tracking, and automatic recovery of stalled jobs.
  </Card>
  <Card title="Tool Dependency Validation" icon="seti:git">
    Declarative tool sequencing prevents the LLM from calling tools in an illogical order.

    **Flow**: LLM generates a dependency graph → admin reviews and approves → system enforces at runtime.

    **Two-Phase**: Pre-flight validation with topological sort → runtime enforcement during execution.
  </Card>
</CardGrid>

## Observability

<CardGrid>
  <Card title="Storyboard UI" icon="seti:graph-line">
    `AgentStoryboardController` powers the LWC components (`agentStoryboard`, `agentExecutionView`, `workflowView`, `agentStoryboardStep`) with a visual timeline of the full execution.
  </Card>
  <Card title="Execution Trace" icon="seti:log">
    `ExecutionStep__c` records capture every interaction — user input, LLM response, tool calls, tool results, and errors — with per-step token counts.
  </Card>
  <Card title="Tool Reasoning" icon="seti:text">
    When `EnableToolReasoning__c` is on, the LLM's rationale for selecting each tool is stored in `ToolRationale__c` for transparency and debugging.
  </Card>
  <Card title="Cost Analytics" icon="seti:db">
    Per-step `PromptTokens__c`, `CompletionTokens__c`, and `EstimatedCostUSD__c` enable budget monitoring dashboards and cost-per-interaction reporting.
  </Card>
</CardGrid>
