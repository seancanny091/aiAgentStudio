---
title: Security
description: Security controls and governance model.
---

import { Card, CardGrid, Steps, Tabs, TabItem, Aside } from '@astrojs/starlight/components';

# Security & Governance

AI Agent Studio implements defense-in-depth security with multiple layers of protection for AI agent operations on Salesforce.

## Security Architecture

<CardGrid>
  <Card title="Platform-Native Security" icon="seti:lock">
    User context execution, CRUD/FLS enforcement, sharing rules, field-level access control with type coercion.
  </Card>
  <Card title="Trust Layers" icon="seti:shield">
    PII masking before LLM calls, provider-native prompt safety pre-checks, tool dependency validation, and declarative sequencing constraints.
  </Card>
  <Card title="Human-in-the-Loop" icon="seti:gear">
    Configurable approval workflows with Confirmation, Approval, and hybrid modes. Atomic state tracking with PendingHITLAction__c.
  </Card>
  <Card title="Audit & Observability" icon="seti:graph-line">
    Complete execution traces, tool rationale capture, decision step logging, token tracking, cost analytics.
  </Card>
</CardGrid>

## Platform-Native Security

### User Context Execution

**No Privilege Escalation**: Agents always run in the context of the user who initiated the execution (`OriginalUserId__c`).

**Sharing Mode**: All classes use `with sharing` or `inherited sharing` to respect Salesforce sharing rules.

**Record-Level Access**: Users can only interact with records they have access to through org-wide defaults, sharing rules, and manual shares.

<Aside type="tip" title="Service User Context">
For scenarios requiring elevated permissions, use `AIAgentDefinition__c.RequiresServiceUserContext__c` to route execution through a dedicated service user via REST API loopback. The original user context is preserved for audit trails.
</Aside>

### CRUD & FLS Enforcement

**Automatic Enforcement**: All SOQL queries use `WITH USER_MODE` to enforce object and field-level security.

```apex
// Framework pattern - always enforces security
List<Account> accounts = [
    SELECT Id, Name, Industry
    FROM Account
    WHERE Id IN :accountIds
    WITH USER_MODE  // Enforces CRUD + FLS
];
```

**DML Security**: All DML operations use `Security.stripInaccessible()` to remove inaccessible fields.

```apex
// Framework pattern for DML
SObjectAccessDecision decision = Security.stripInaccessible(
    AccessType.CREATABLE,
    recordsToInsert
);
insert decision.getRecords();
```

**Type Coercion with FLS**: `TypeCoercionService.coerceArgumentTypesForSObject()` validates field access when converting LLM-provided arguments to SObject field values.

### Permission Checks

**Object Permissions**: `Utils.checkObjectPermission()` validates CRUD access before operations.

```apex
// Validate read access before querying
Utils.checkObjectPermission(
    Account.SObjectType,
    AccessType.READABLE
);
```

**Field Accessibility**: Framework respects field-level security when building SOQL queries and processing DML operations.

## Trust Layers

<Tabs>
  <TabItem label="PII Masking">

### Hybrid PII Masking

Prevents sensitive data from reaching LLM providers in raw form.

**Architecture**: `PIIMaskingService` orchestrates `SchemaBasedMasker` (Salesforce Data Classification) and `PIIPatternMatcher` (regex patterns).

**How It Works**:

1. **Masking Phase**: User message → scan for PII → replace with deterministic tokens (`[SSN:001]`)
2. **LLM Processing**: Masked message sent to LLM with tokens instead of actual values
3. **Unmasking Phase**: LLM response → replace tokens with original values → return to user
4. **Bidirectional**: Applies to both user messages and tool arguments in both directions

**Configuration**:

Per-agent via `AIAgentDefinition__c`:
- `PIIMaskingPreset__c`: Off / Standard / Strict
  - Standard: Hybrid masking with common categories and classifications
  - Strict: Hybrid masking with all categories and classifications

**Pattern Coverage**:
- **SSN**: `###-##-####` format with validation
- **Credit Cards**: Luhn algorithm validation for card numbers
- **Email**: RFC-compliant email address detection
- **Phone**: US and international phone number formats
- **IPv4**: IP address detection
- **DOB**: Date of birth in various formats

**Key Features**:
- **Deterministic Tokens**: Same value always gets same token within session
- **No Persistence**: Mapping stored only in memory for session scope
- **FLS Respected**: Schema-based masking honors field-level security
- **Tool Arguments**: Applies to both user input and LLM tool call arguments

**Example**:

```
User: "Update case for customer SSN 123-45-6789"
Masked: "Update case for customer SSN [SSN:001]"
→ LLM processes masked version
→ Tool execution receives unmasked value
→ Response shown to user with original values
```

  </TabItem>
  <TabItem label="Prompt Safety">

### Provider-Native Safety Pre-Checks

Prompt safety is delegated to each LLM provider adapter, ensuring the check matches the provider's own safety semantics rather than a generic pattern-matching layer.

**Architecture**: `LLMInteractionService.prepareAndCallLLM()` calls `ILLMProviderAdapter.checkMessageSafety()` **before** the LLM call. `BaseProviderAdapter` returns `null` by default (no pre-check). Adapters that support a safety API override this method.

**Flow**:
```
LLMInteractionService.prepareAndCallLLM()
  → adapter.checkMessageSafety()   // pre-call; null = no pre-check
  → [if ThreatAssessment returned and shouldBlock()] → log + return safe error
  → adapter.sendMessage()          // LLM call
  → [if ProviderResult.safetyBlock != null] → log + return safe error
  → normal response processing
```

**Provider Behaviours**:

| Provider | Pre-Check | Notes |
|----------|-----------|-------|
| `OpenAIProviderAdapter` | Calls `/v1/moderations` — blocks if `flagged == true` | Provider-native; also works for any OpenAI-compatible API that exposes `/v1/moderations` |
| `VertexAIProviderAdapter` | None (inherits null default) | Relies on Vertex AI's in-call safety filters instead |
| Custom adapters | Override `checkMessageSafety()` to add | Fully extensible |

**Result DTO**: `ThreatAssessment` carries `score` (0.0–1.0), `threatLevel` (NONE/LOW/MEDIUM/HIGH/CRITICAL), `indicators` list, and `shouldBlock()` helper.

**In-Call Safety**: Adapters can also set `ProviderResult.safetyBlock` when a post-response safety signal is detected (e.g., a refusal `finish_reason`). `LLMInteractionService` checks this after `sendMessage()` and short-circuits to a safe response when non-null.

**Configuration**: Per-agent via `AIAgentDefinition__c.PromptSafetyPreset__c`:
- `Off`: Skips the pre-check step entirely
- `Standard`: Adapter performs its native check and blocks on positive signal

</TabItem>
  <TabItem label="Tool Dependencies">

### Declarative Tool Sequencing

Prevents workflow hallucinations where LLM calls tools in illogical order.

**Problem**: Without constraints, LLM might call `send_email` before `create_record`, or `update_record` before `get_record_details`.

**Solution**: Shadow Graph Pattern - LLM generates dependency graph, admin approves, system enforces at runtime.

**How It Works**:

1. **Graph Generation**: `ToolDependencyGraphService` uses LLM to analyze agent capabilities and suggest dependency graph
2. **Admin Review**: Human reviews and edits graph in `ToolDependencyGraphEditorController` UI
3. **Storage**: Approved graph stored in `AIAgentDefinition__c.ToolDependencyGraph__c` as JSON
4. **Runtime Enforcement**: `ToolDependencyValidator` checks dependencies before tool execution

**Dependency Logic**:

```json
{
  "version": "1.0",
  "dependencies": {
    "update_record": {
      "allOf": ["get_record_details"]
    },
    "send_email": {
      "allOf": ["update_record"],
      "anyOf": ["get_email_address", "get_contact_info"]
    }
  }
}
```

- `allOf`: ALL tools must be executed first (AND logic)
- `anyOf`: AT LEAST ONE tool must be executed first (OR logic)
- Combined: `send_email` requires `update_record` AND (at least one of `get_email_address` OR `get_contact_info`)

**Two-Phase Validation**:

1. **Pre-Flight Validation** (before executing any tools in batch):
   - Validates ALL tools in batch
   - Attempts intelligent reordering via topological sort
   - Only blocks tools whose dependencies are completely missing from batch
   - Keeps transaction clean (no DML before validation failure)

2. **Runtime Validation** (during execution loop):
   - Re-validates dependencies at execution time
   - Catches cases where dependency in same batch failed
   - Only successful tools satisfy dependencies

**Circuit Breaker**: `ToolCallResponseHandler` tracks total dependency violations across execution. If threshold exceeded (default 10, configurable via `AIAgentFrameworkSettings__c.MaxDependencyViolations__c`), fails execution immediately to prevent infinite loops.

**LLM Guidance on Violation**: When tool blocked, system provides structured error message explaining required dependencies and next action.

**Configuration**: Enable via `AIAgentDefinition__c.EnableDependencyValidation__c`

**Limitations**: Only enforces synchronous tools in same batch. Async tools (separate jobs) cannot have dependencies enforced. Scope is turn-scoped for Conversational/Email agents (reset each turn) and execution-scoped for Function/Workflow agents.

  </TabItem>
</Tabs>

## Human-in-the-Loop (HITL)

### Approval Workflows

Configurable approval requirements for sensitive actions via `AgentCapability__c.HITLMode__c`.

<CardGrid>
  <Card title="Disabled" icon="approve-check">
    No HITL. Action executes immediately when the LLM calls it.
  </Card>
  <Card title="Confirmation" icon="chat">
    LLM asks the user for confirmation in chat before executing. No formal approval record created.
  </Card>
  <Card title="Approval" icon="seti:settings">
    Formal approval process via `PendingHITLAction__c`. Execution pauses until an approver acts. Notification sent based on `HITLNotificationPreference__c`.
  </Card>
  <Card title="ConfirmationThenApproval" icon="warning">
    Requires both in-chat confirmation from the user AND a formal approval. Highest gate for critical operations.
  </Card>
</CardGrid>

**Notification Preferences** (`HITLNotificationPreference__c`): **Always Notify** sends notifications for approvals, rejections, and errors (default). **Notify on Rejection Only** only sends notifications when actions are rejected.

### Approval State Management

**Object**: `PendingHITLAction__c` tracks approval state with atomic locking.

**Lifecycle**: Action requires approval → Create PendingHITLAction__c record → Set ExecutionStatus__c to 'Awaiting Action' → Notify approver (if configured) → Approver reviews and approves/rejects → On approval: Execute action and update execution → On rejection: Log rejection and mark execution failed/cancelled.

**Security**: Approvers must have access to source record and capability to approve.

## Audit & Observability

### Execution Traces

Every agent execution produces a detailed, immutable log capturing:
- User input, LLM requests/responses, tool calls, tool results, and errors
- Token counts (prompt, completion, total) and estimated cost per step
- Tool rationale when `EnableToolReasoning__c` is enabled

The **Execution Storyboard** UI surfaces a user-friendly decision timeline from this data — showing high-level steps, success/failure indicators, and tool rationale for transparency.

### Tool Reasoning & Explainability

When `AIAgentDefinition__c.EnableToolReasoning__c` is enabled:

1. The framework adds a required `_rationale` parameter to all tools in the LLM schema
2. The LLM must provide its reasoning for each tool call
3. The rationale is extracted and stored separately from the tool arguments
4. Displayed in the Execution Storyboard for user visibility and audit

<CardGrid>
  <Card title="Explainability" icon="seti:text">
    End users can see exactly why the agent chose each action, building trust and transparency.
  </Card>
  <Card title="Debugging" icon="seti:log">
    Developers can inspect LLM reasoning for unexpected tool selections during development and support.
  </Card>
  <Card title="Compliance" icon="seti:lock">
    Audit trails include decision rationale, satisfying compliance requirements for automated AI decisions.
  </Card>
  <Card title="Accuracy" icon="approve-check">
    Forces the LLM to reason through tool selection before committing, which measurably improves tool call quality.
  </Card>
</CardGrid>

### Token Tracking & Cost Analytics

**Per-Step Tracking**: Each step records input tokens, output tokens, total tokens, and estimated cost based on model pricing.

**Aggregation**: Build dashboards to track:
- Total cost per agent, per user, per day
- Token efficiency (tokens per conversation turn)
- Cost trends over time
- Identify high-cost agents for optimization

## Best Practices

<Steps>

1. **Start in Sandbox**
   
   Deploy to sandbox first with representative data. Test with various user profiles to validate CRUD/FLS enforcement.

2. **Principle of Least Privilege**
   
   Create dedicated integration users with minimal permissions needed. Don't grant system admin to agent service users.

3. **Enable Trust Layers Incrementally**
   
   Start with `PIIMaskingPreset__c = Standard` and `PromptSafetyPreset__c = Off` in your first sandbox deployment. Monitor agent behavior, then enable `PromptSafetyPreset__c = Standard` once you've validated the provider's safety checks are not producing false positives.

4. **Route Sensitive Actions Through Approvals**
   
   Use HITL Approval mode for data deletion, external integrations, financial transactions, and high-impact operations.

5. **Monitor Execution Anomalies**
   
   Build dashboards on execution data. Alert on:
   - High failure rates
   - Sudden token cost spikes
   - Prompt safety violations
   - HITL rejection rates

6. **Review Tool Dependencies**
   
   Use `ToolDependencyGraphService` to generate initial graph, but have domain experts review and refine before production.

7. **Regular Audit Reviews**
   
   Schedule periodic reviews of:
   - Tool rationale for unexpected patterns
   - Prompt safety flags
   - PII masking effectiveness
   - HITL approval/rejection trends

</Steps>

## Security Checklist

<Aside type="caution" title="Production Deployment">
Always test security controls in sandbox with production data volumes before deploying to production. Monitor closely during initial rollout and be prepared to disable agents quickly if issues arise.
</Aside>

<CardGrid>
  <Card title="Platform Security" icon="seti:lock">
    - [ ] Agents run as users with appropriate profiles/permission sets
    - [ ] Service user context configured (if needed) with minimal permissions
    - [ ] All custom actions use `WITH USER_MODE` in SOQL
    - [ ] DML operations use `Security.stripInaccessible()`
  </Card>
  <Card title="Trust Layers" icon="seti:shield">
    - [ ] PII masking enabled (`Standard` or `Strict`) with appropriate classifications
    - [ ] Prompt safety preset set to `Standard` for production agents using a provider with native safety support
    - [ ] Tool dependency validation enabled for agents with ordered workflows
    - [ ] Dependency graph reviewed by domain experts before enabling
  </Card>
  <Card title="Human-in-the-Loop" icon="approve-check">
    - [ ] Sensitive actions (deletions, financial ops, external integrations) routed through HITL
    - [ ] Approval notification preferences configured
    - [ ] Approver access to source records verified
    - [ ] User training completed on HITL approval responsibilities
  </Card>
  <Card title="Monitoring & Audit" icon="seti:graph-line">
    - [ ] Dashboard/alerts configured for execution monitoring
    - [ ] Audit trail retention policy established for `ExecutionStep__c`
    - [ ] Token budget limits and cost alerts configured
    - [ ] High failure rate and HITL rejection rate alerts set up
  </Card>
</CardGrid>
